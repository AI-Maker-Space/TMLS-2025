{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "<h1 align=\"center\" id=\"heading\">RAG in Practice - 2025</h1>\n",
    "\n",
    "In this event, we'll be looking at best-practice tools for RAG in 2025.\n",
    "\n",
    "We'll go over how to build a solid baseline RAG application (essentially what is tablestakes as of May 14th, 2025). \n",
    "\n",
    "This is by no means the *end* of the line for RAG - but it is what you should be building *at minimum* as you create RAG applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also select a best practice visibility and monitoring tool [Arize Phoenix](https://docs.arize.com/phoenix)\n",
    "\n",
    "> Instructions for launching the local Arize Pheonix deployment is available [here](https://docs.arize.com/phoenix/tracing/integrations-tracing/langgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔭 OpenTelemetry Tracing Details 🔭\n",
      "|  Phoenix Project: rag-in-practice-2025\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ⚠️ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n",
    "\n",
    "tracer_provider = register(\n",
    "  project_name=\"rag-in-practice-2025\",\n",
    ")\n",
    "\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we're not using LangChain's default splitters. While this example is simplified - the idea holds true: \n",
    "\n",
    "Chunking/splitting is something that should be done in a thought-out way, relying on base chunking strategies is not a suggested practice. \n",
    "\n",
    "Since our data is quotations separated by `\\n\\n` - we'll build a simple function that splits our data into coherent chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(doc_path: str) -> list:\n",
    "    # Read the document\n",
    "    with open(doc_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split the document by double newlines\n",
    "    chunks = content.split('\\n\\n')\n",
    "    \n",
    "    # Remove any empty chunks\n",
    "    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_vinci_documents = split_docs(\"./data/da_vinci.txt\")\n",
    "len(da_vinci_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three classes of people: those who see. Those who see when they are shown. Those who do not see.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_vinci_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "henry_david_thoreau_documents = split_docs(\"./data/henry_david_thoreau.txt\")\n",
    "len(henry_david_thoreau_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. “All good things are wild and free.”'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "henry_david_thoreau_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - our Thoreau quotations have distracting characters - we'll clean those. \n",
    "\n",
    "> NOTE: Data quality is still incredibly important - again, this is a simple example - but it's meant to demonstate that you should care about your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_hdt_quotations(list_of_quotations):\n",
    "    cleaned_quotations = []\n",
    "    \n",
    "    for quotation in list_of_quotations:\n",
    "        without_numbering = re.sub(r'^\\d+\\.\\s+', '', quotation)\n",
    "        \n",
    "        cleaned_quote = without_numbering.strip('”')\n",
    "        if cleaned_quote.startswith('“'):\n",
    "            cleaned_quote = cleaned_quote[1:]\n",
    "        \n",
    "        cleaned_quotations.append(cleaned_quote)\n",
    "    \n",
    "    return cleaned_quotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All good things are wild and free.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "henry_david_thoreau_documents_clean = clean_hdt_quotations(henry_david_thoreau_documents)\n",
    "henry_david_thoreau_documents_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model\n",
    "\n",
    "We'll be using [llama-3.2-nv-embedqa-1b-v2](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2) as our Embedding Model today - it's fast, efficient, has long context, supports multiple languages, has incredibly high scores for retrieval tasks, and can be securely and locally hosted as a NIM. \n",
    "\n",
    "> Instructions on launching the NIM are available [here](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2/deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "nvidia_embeddings = NVIDIAEmbeddings(\n",
    "  base_url=\"http://192.168.2.42:8000/v1\",\n",
    "  model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", \n",
    "  truncate=\"NONE\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nvidia_embeddings.embed_query(\"Hello, there!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vector Store - Qdrant\n",
    "\n",
    "Qdrant is our recommended Vector Database as it serves as a tool that you can start with, and will scale with you as your needs grow. \n",
    "\n",
    "> Instructions on how to get QDrant spun up are available [here](https://qdrant.tech/documentation/quickstart/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/Code/AI Makerspace/Events/RAG-In-Practice-2025/.venv/lib/python3.13/site-packages/qdrant_client/http/models/models.py:758: SyntaxWarning: invalid escape sequence '\\&'\n",
      "  description=\"Check that the field is empty, alternative syntax for `is_empty: \\&quot;field_name\\&quot;`\",\n",
      "/home/chris/Code/AI Makerspace/Events/RAG-In-Practice-2025/.venv/lib/python3.13/site-packages/qdrant_client/http/models/models.py:762: SyntaxWarning: invalid escape sequence '\\&'\n",
      "  description=\"Check that the field is null, alternative syntax for `is_null: \\&quot;field_name\\&quot;`\",\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"quotation_collection\",\n",
    "    vectors_config=VectorParams(size=2048, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"quote_collection\",\n",
    "    embedding=nvidia_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Notice how we inject metadata into Qdrant - this can be leveraged directly or indirectly as we'll see when we use the application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "uuids = []\n",
    "\n",
    "for i, quote in enumerate(henry_david_thoreau_documents_clean):\n",
    "    document = Document(\n",
    "        page_content=quote,\n",
    "        metadata={\"author\": f\"Henry David Thoreau\"},\n",
    "    )\n",
    "    uuid = str(uuid4())\n",
    "    documents.append(document)\n",
    "    uuids.append(uuid)\n",
    "\n",
    "_ = vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "uuids = []\n",
    "\n",
    "for i, quote in enumerate(da_vinci_documents):\n",
    "    document = Document(\n",
    "        page_content=quote,\n",
    "        metadata={\"author\": f\"Leonardo da Vinci\"},\n",
    "    )\n",
    "    uuid = str(uuid4())\n",
    "    documents.append(document)\n",
    "    uuids.append(uuid)\n",
    "\n",
    "_ = vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Retriever\n",
    "\n",
    "We are going to recommend that for retrieval you begin with: \n",
    "\n",
    "1. Hybrid Retrieval (Dense + Keyword)\n",
    "2. Reranking \n",
    "\n",
    "While methods like HyDE can be leveraged to great effect in some situations (as was shown in [this paper](https://arxiv.org/pdf/2407.01219)) - they're not as universally useful as the above flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Retrieval\n",
    "\n",
    "We'll combine our dense retreival (LLM Embeddings) with BM25 and combine them with reciprocal rank-fusion (rRF) to produce a final list. \n",
    "\n",
    "> NOTE: We are choosing 5 pieces of context from both retrieval processes - this is an adjustable hyperparameter. The number should be larger than your final desired number of documents since the Reranking process can be used to filter out the lowest scoring options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    texts=henry_david_thoreau_documents_clean+da_vinci_documents,\n",
    ")\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vector_store.as_retriever(search_kwargs={\"k\": 5}), \n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.5, 0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking\n",
    "\n",
    "Reranking is an extremely important piece of the puzzle - as it allows us to more deeply search through a subset of our documents. \n",
    "\n",
    "> Instructions on how to run the NIM are available [here](https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2/deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_nvidia_ai_endpoints.reranking import NVIDIARerank\n",
    "\n",
    "cohere_rerank = NVIDIARerank(\n",
    "    base_url=\"http://192.168.2.42:8002/v1\",\n",
    "    model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    ")\n",
    "\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_rerank, base_retriever=hybrid_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Prompt\n",
    "\n",
    "Prompts are still crucial - and while this prompt works well for this use-case, time should still be spent on exactly how you're prompting your LLM based on a number of factors. (generator model being used, number of documents retrieved, etc.)\n",
    "\n",
    "This process is, like data ingestion, something to spend time and effort on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", RAG_TEMPLATE)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "While we're recommending [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) as the \"default best RAG model\" - this is another choice that is use-case, domain, and cost based.\n",
    "\n",
    "Right-sizing your model is still a difficult process in 2025 - but for most use-cases where you need a good model and can afford/support Llama 3.3 70B Instruct - you should find it extremely effective. \n",
    "\n",
    "> NOTE: For this example, so that it can run locally on most systems, we're going to be running `ollama` with the recent `qwen3:14b` - instructions on how to get this running are available [here](https://ollama.com/library/qwen3)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:14b\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to figure out what Thoreau said about the importance of solitude. Let me start by recalling what I know about Henry David Thoreau. He was a 19th-century American author, philosopher, and transcendentalist. He\\'s best known for his book \"Walden,\" where he describes his experience living in a cabin near Walden Pond. I think he emphasized living simply and in harmony with nature.\\n\\nNow, solitude. Thoreau probably talked about it in \"Walden.\" I remember something about him valuing solitude for self-reflection and personal growth. Maybe he believed that being alone allows one to think more clearly or to escape the distractions of society. But I need to be more specific.\\n\\nI should check some quotes from \"Walden.\" One famous line is \"I went to the woods because I wished to live deliberately, to front only the essential facts of life...\" That\\'s from the beginning. But how does that relate to solitude? Well, living in the woods would mean solitude, so maybe he\\'s saying that solitude is necessary to live a meaningful life.\\n\\nAnother part I recall is where he talks about the importance of solitude for understanding oneself. Maybe he said something like solitude helps you discover your true self or that society\\'s noise prevents people from thinking deeply. There\\'s a quote about how people are too busy with their daily lives to think about important things, and solitude allows for that reflection.\\n\\nWait, there\\'s a quote where he mentions that \"the mass of men lead lives of quiet desperation,\" which might be related. If people are in solitude, they can avoid that desperation by living more authentically. Also, he might have compared solitude to the natural world, suggesting that being alone in nature allows for a deeper connection with the self and the environment.\\n\\nI think he also talked about how solitude is not loneliness but a choice to be alone for the sake of introspection. Maybe he believed that solitude is essential for creativity or for making important life decisions. In \"Walden,\" he often contrasts the simplicity of life in the woods with the complexity and artificiality of city life, implying that solitude in nature is where true simplicity and clarity of thought can be found.\\n\\nI should also consider his other works, like \"Civil Disobedience,\" but I think that\\'s more about resistance to government rather than solitude. So focusing on \"Walden\" is better. Let me try to remember specific quotes. There\\'s one where he says something like \"solitude is not loneliness; it is the presence of the self.\" Or maybe \"solitude is the only place where one can be truly free.\"\\n\\nAnother angle: Thoreau might have argued that solitude allows individuals to break free from societal expectations and find their own path. He valued individualism, so solitude would be a way to cultivate that. Also, he might have seen solitude as a way to confront one\\'s own mortality and the impermanence of life, leading to a more meaningful existence.\\n\\nWait, I think he wrote about how people are often too preoccupied with material things and not enough with their inner lives. Solitude, by removing them from that noise, allows them to focus on what\\'s truly important. Maybe he also mentioned that solitude is necessary for spiritual growth or for understanding the deeper truths of life.\\n\\nI should also check if there are any specific chapters in \"Walden\" that discuss solitude. The chapter \"Solitude\" might be a direct reference. If I recall correctly, in that chapter he talks about how he found companionship in solitude and that it\\'s not about being alone but about being in the presence of the natural world and one\\'s own thoughts.\\n\\nAnother point: Thoreau might have believed that solitude is a form of resistance against the conformity of society. By choosing solitude, one resists the pressures to conform and instead lives according to one\\'s own values. This ties into his transcendentalist beliefs, which emphasize individualism and the importance of personal experience.\\n\\nI think he also mentioned that solitude allows for a more direct experience of nature, which in turn leads to a greater understanding of oneself and the universe. So solitude is both a personal and a spiritual practice for him.\\n\\nTo sum up, Thoreau likely emphasized solitude as a means to self-discovery, personal growth, and a deeper connection with nature and the self. He probably saw it as essential for living authentically and for resisting the corrupting influences of society. Specific quotes would be key here, so I need to make sure I get those right.\\n</think>\\n\\nHenry David Thoreau, in his seminal work *Walden* (1854), emphasized the transformative power of solitude as a means to self-discovery, authenticity, and a deeper connection with nature and the self. Here are key points and quotes that reflect his views on solitude:\\n\\n1. **Solitude as a Path to Self-Discovery**:  \\n   Thoreau believed solitude allows individuals to confront their true selves and break free from societal expectations. He wrote:  \\n   > *\"I have never found the companion that was so companionable as solitude.\"*  \\n   This reflects his view that solitude is not loneliness but a source of profound inner companionship and clarity.\\n\\n2. **Resistance to Conformity**:  \\n   Solitude, for Thoreau, was a form of resistance against the \"quiet desperation\" of mass society. He argued that societal pressures often lead people to live inauthentically:  \\n   > *\"The mass of men lead lives of quiet desperation.\"*  \\n   By choosing solitude, individuals can escape the noise of conformity and live deliberately, aligning their lives with their values.\\n\\n3. **Connection with Nature**:  \\n   Thoreau saw solitude in nature as essential for spiritual and intellectual growth. He wrote:  \\n   > *\"In the long run, the more solitary a man is, the more he is himself.\"*  \\n   Living in the woods near Walden Pond, he found that solitude in nature fostered a deeper understanding of the self and the universe.\\n\\n4. **Simplicity and Clarity of Thought**:  \\n   Thoreau believed solitude allows for reflection and clarity, free from the distractions of modern life. He stated:  \\n   > *\"I went to the woods because I wished to live deliberately, to front only the essential facts of life...\"*  \\n   This deliberate living, achieved through solitude, enabled him to focus on what truly matters.\\n\\n5. **Spiritual and Philosophical Growth**:  \\n   Solitude was a means to confront mortality and the impermanence of life, leading to a more meaningful existence. He wrote:  \\n   > *\"Solitude is not loneliness; it is the presence of the self.\"*  \\n   This presence of self, cultivated in solitude, allowed for introspection and a deeper engagement with life\\'s essential truths.\\n\\n6. **Individualism and Freedom**:  \\n   Thoreau viewed solitude as a way to cultivate individualism and freedom from societal constraints. He argued that solitude is where one can \"live according to one\\'s own values\" and resist the \"corrupting influences\" of conformity.\\n\\nIn essence, Thoreau saw solitude not as isolation but as a vital practice for living authentically, fostering self-awareness, and deepening one\\'s connection with nature and the self. His reflections in *Walden* continue to resonate as a call to embrace solitude as a path to a more meaningful and deliberate life.', additional_kwargs={}, response_metadata={'model': 'qwen3:14b', 'created_at': '2025-05-14T16:53:12.991231731Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14124289844, 'load_duration': 8510896, 'prompt_eval_count': 20, 'prompt_eval_duration': 17842673, 'eval_count': 1523, 'eval_duration': 14097206769, 'model_name': 'qwen3:14b'}, id='run--ae1477e0-ae72-41ca-86e3-498b10d5e0a3-0', usage_metadata={'input_tokens': 20, 'output_tokens': 1523, 'total_tokens': 1543})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What did Thoreau say about the importance of solitude?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RAG Graph\n",
    "\n",
    "We continue to suggest people to use LangGraph in 2025. Notice that LCEL is largely removed as the core way to interact with the LangX ecosystem - as we prefer to interact with it through LangGraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: There is a subtle best-practice in the next cell, which is to return your contexts in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: RAGState) -> RAGState:\n",
    "  retrieved_docs = reranker.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs[::-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "  generation_chain = chat_prompt | llm | StrOutputParser()\n",
    "  response = generation_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
    "  return {\"response\" : response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(RAGState)\n",
    "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Present in Documents\n",
    "\n",
    "In this case, we know the question should not be answerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking why birds sing. Let me check the provided context to see if there's any relevant information.\n",
      "\n",
      "Looking through the documents: the first one is about treating virtuous people with honor, the second is Leonardo da Vinci talking about people in hot and cold countries and their color preferences, the third and fourth are about understanding oneself when lost, and the fifth is Thoreau's quote about people leading quiet desperate lives. None of these documents mention birds or their singing. \n",
      "\n",
      "Since there's no information about birds in the context provided, I can't use any of these to answer the query. The correct response is to say I don't know.\n",
      "\n",
      "\n",
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"Why do birds sing?\"})\n",
    "thinking, response = response[\"response\"].split(\"</think>\")\n",
    "\n",
    "print(thinking)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong Author\n",
    "\n",
    "In this case, we know Da Vinci didn't have quotes about solitude in the corpus we used - but Thoreau did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Da Vinci had to say about solitude. Let me check the provided context.\n",
      "\n",
      "Looking at the documents, there's one attributed to Leonardo da Vinci. The page content is a quote where he says, \"Study me, reader... because the patience for this profession is found in very few, and only in those who wish to compose things anew. Come, oh men, to see the miracles that such studies will disclose to nature.\" \n",
      "\n",
      "Wait, this quote is more about dedication to study and creating new things, not directly about solitude. The other documents include quotes about solitude, like Thoreau's, but the user specifically asked about Da Vinci. Since the Da Vinci quote doesn't mention solitude, I can't use it to answer the query. The other quotes are from different authors. So, there's no information in the context about Da Vinci's views on solitude. I should respond that I don't know.\n",
      "\n",
      "\n",
      "\n",
      "I don't know. The provided context includes a quote from Leonardo da Vinci about dedication to study and creation, but it does not mention solitude or Da Vinci's views on it.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"What did Da Vinci have to say about solitude?\"})\n",
    "thinking, response = response[\"response\"].split(\"</think>\")\n",
    "\n",
    "print(thinking)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'relevance_score': -20.9375}, page_content='What lies behind us and what lies ahead of us are tiny matters compared to what lives within us.'),\n",
       " Document(metadata={'relevance_score': -20.75}, page_content='Life isn’t about finding yourself; it’s about creating yourself. So live the life you imagined.'),\n",
       " Document(metadata={'relevance_score': -19.40625}, page_content='It is not enough to be busy. So are the ants. The question is: What are we busy about?'),\n",
       " Document(metadata={'author': 'Leonardo da Vinci', '_id': 'e2946ebf-9946-4f30-b136-6d9d03460414', '_collection_name': 'quote_collection', 'relevance_score': -15.7109375}, page_content='\"Study me, reader, if you delight in me, because on very few occasions shall I return to the world, and because the patience for this profession is found in very few, and only in those who wish to compose things anew. Come, oh men, to see the miracles that such studies will disclose to nature.\"'),\n",
       " Document(metadata={'author': 'Henry David Thoreau', '_id': '8a818fbf-d235-494d-8c52-977b5c507a1a', '_collection_name': 'quote_collection', 'relevance_score': -10.46875}, page_content='I find it wholesome to be alone the greater part of the time. To be in company, even with the best, is soon wearisome and dissipating. I love to be alone. I never found the companion that was so companionable as solitude.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present in Documents\n",
    "\n",
    "Of course, the system should work for cases where the documentation exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Thoreau had to say about solitude. Let me check the provided context.\n",
      "\n",
      "Looking at the documents, there are two quotes from Thoreau. The first one is about going to the woods to live deliberately. The second one says, \"I find it wholesome to be alone the greater part of the time. To be in company, even with the best, is soon wearisome and dissipating. I love to be alone. I never found the companion that was so companionable as solitude.\"\n",
      "\n",
      "That second quote directly addresses solitude. The user's question is specifically about solitude, so that's the relevant part. The other quotes are from different authors or not about solitude. So the answer should be that Thoreau believed solitude is wholesome and more companionable than being with others. I need to make sure not to include other quotes unless they're relevant. The answer is in the second Thoreau quote provided.\n",
      "\n",
      "\n",
      "\n",
      "Thoreau stated, \"I find it wholesome to be alone the greater part of the time. To be in company, even with the best, is soon wearisome and dissipating. I love to be alone. I never found the companion that was so companionable as solitude.\" He viewed solitude as a source of fulfillment and a more meaningful companion than social interaction.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"What did Thoreau have to say about solitude?\"})\n",
    "thinking, response = response[\"response\"].split(\"</think>\")\n",
    "\n",
    "print(thinking)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'relevance_score': -14.9609375}, page_content='Life isn’t about finding yourself; it’s about creating yourself. So live the life you imagined.'),\n",
       " Document(metadata={'relevance_score': -13.25}, page_content='What lies behind us and what lies ahead of us are tiny matters compared to what lives within us.'),\n",
       " Document(metadata={'relevance_score': -11.234375}, page_content='It is not enough to be busy. So are the ants. The question is: What are we busy about?'),\n",
       " Document(metadata={'author': 'Henry David Thoreau', '_id': '9ceacdad-2d22-4258-b0b0-e41eedff401d', '_collection_name': 'quote_collection', 'relevance_score': -4.08203125}, page_content='I went to the woods because I wished to live deliberately, to front only the essential facts of life, and see if I could not learn what it had to teach, and not, when I came to die, discover that I had not lived.'),\n",
       " Document(metadata={'author': 'Henry David Thoreau', '_id': '8a818fbf-d235-494d-8c52-977b5c507a1a', '_collection_name': 'quote_collection', 'relevance_score': -2.375}, page_content='I find it wholesome to be alone the greater part of the time. To be in company, even with the best, is soon wearisome and dissipating. I love to be alone. I never found the companion that was so companionable as solitude.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizing Responses\n",
    "\n",
    "Asking about quotes that are similar should allow us to compare and contrast each author's thoughts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking about what Da Vinci and Thoreau said regarding perception or seeing, and to compare and contrast their thoughts. I need to use the provided context.\n",
      "\n",
      "Looking at the documents, there's a quote from Da Vinci: \"Drawing is based upon perspective, which is nothing else than a thorough knowledge of the function of the eye.\" So he's talking about perspective in drawing being related to understanding how the eye works. That seems to tie into perception, as perspective is about how we see things in art.\n",
      "\n",
      "Then there's Thoreau's quote: \"It’s not what you look at that matters, it’s what you see.\" This is more about the importance of perception beyond just the physical act of looking. He's emphasizing that seeing involves deeper understanding or insight, not just visual observation.\n",
      "\n",
      "Comparing them: Da Vinci focuses on the technical aspect of perception, specifically the eye's function in creating accurate perspective. Thoreau is more philosophical, saying that true seeing is about interpretation and insight rather than just the object being observed. \n",
      "\n",
      "Contrasting them: Da Vinci's view is more about the mechanics of vision, while Thoreau's is about the subjective interpretation. Da Vinci is practical (art and perspective), Thoreau is about personal insight and deeper understanding. \n",
      "\n",
      "I need to make sure I don't use any other information outside the provided context. The other quotes are from unknown authors and not relevant here. So the answer should focus on these two quotes and their comparison.\n",
      "\n",
      "\n",
      "\n",
      "Leonardo da Vinci and Henry David Thoreau each offered distinct perspectives on perception and seeing, as reflected in their quotes:\n",
      "\n",
      "1. **Leonardo da Vinci** emphasized the technical and scientific foundation of perception:  \n",
      "   *\"Drawing is based upon perspective, which is nothing else than a thorough knowledge of the function of the eye.\"*  \n",
      "   Here, da Vinci links seeing to the mechanics of vision, framing perception as a measurable, objective process rooted in understanding how the eye interprets spatial relationships (e.g., perspective in art).\n",
      "\n",
      "2. **Henry David Thoreau** focused on the subjective and interpretive nature of seeing:  \n",
      "   *\"It’s not what you look at that matters, it’s what you see.\"*  \n",
      "   Thoreau argues that true perception transcends mere visual observation, requiring deeper insight, interpretation, and awareness of meaning beyond the surface.\n",
      "\n",
      "**Comparison**: Both address the act of seeing, but da Vinci approaches it through a lens of technical precision (the eye’s function), while Thoreau highlights the internal, interpretive process of perception.  \n",
      "\n",
      "**Contrast**: Da Vinci’s view is objective and analytical, tied to the physical mechanics of vision, whereas Thoreau’s is subjective and philosophical, emphasizing the importance of inner understanding over external observation.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"What did Da Vinci and Thoreau say about perception/seeing? Compare and contrast their thoughts.\"})\n",
    "thinking, response = response[\"response\"].split(\"</think>\")\n",
    "\n",
    "print(thinking)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution\n",
    "\n",
    "For Evaluating RAG applications in 2025 - we continue to recommend [Ragas](https://github.com/explodinggradients/ragas/tree/main) as a best practice tool. \n",
    "\n",
    "> NOTE: Please see our [Ragas event](https://www.youtube.com/watch?v=bB56BaQIBm4) for more in-depth explanations and examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpasss\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Provide your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import AspectCritic\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": \"What did Da Vinci and Thoreau say about perception/seeing? Compare and contrast their thoughts.\",\n",
    "    \"response\": response[\"response\"],\n",
    "}\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ### INSERT YOUR OSS MODEL HERE ###\n",
    ")\n",
    "metric = AspectCritic(name=\"fully_answered\",llm=evaluator_llm, definition=\"Verify if the question was answered completely.\")\n",
    "await metric.single_turn_ascore(SingleTurnSample(**test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
